networks:
  iceberg_network:
    driver: bridge

services:
  minio:
    image: minio/minio:RELEASE.2024-10-13T13-34-11Z
    container_name: minio
    ports:
      - "9200:9000"   # MinIO API
      - "9201:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    networks:
      iceberg_network:
        aliases:
          - messud-bucket.minio

  iceberg_rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: iceberg_rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      AWS_ENDPOINT: http://minio:9000

      CATALOG_REST_URI: http://iceberg_rest:8181/
      CATALOG_REST_WAREHOUSE: s3://messud-bucket/
      CATALOG_REST_IO_IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_REST_S3_ENDPOINT: http://minio:9000
      CATALOG_REST_S3_ACCESS-KEY-ID: minioadmin
      CATALOG_REST_S3_SECRET-ACCESS-KEY: minioadmin
    depends_on:
      - minio
    networks:
      iceberg_network:

  airflow-db-project:
    container_name: airflow-db-project
    image: postgres:16.4
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - pgdata_airflow-project:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 5
    restart: always
    networks:
      iceberg_network:

  clickhouse-server-project:
    image: clickhouse/clickhouse-server
    container_name: clickhouse-server-project
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data-project:/var/lib/clickhouse/
      - ./sample_data:/var/lib/clickhouse/user_files
      - ./sql:/sql
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: "12345678"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      S3_ENDPOINT: http://minio:9000
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8123/ping || exit 1"]
      interval: 10s
      retries: 10
    networks:
      iceberg_network:
    restart: always

  airflow-init-project:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-init-project
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || true
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db-project:5432/airflow
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: clickhouse+native://default:12345678@clickhouse-server-project:9000/default
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./sample_data:/var/lib/clickhouse/user_files
    depends_on:
      - airflow-db-project
      - clickhouse-server-project
    restart: "no"
    networks:
      iceberg_network:

  airflow-webserver-project:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver-project
    command: webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db-project:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: clickhouse+native://default:12345678@clickhouse-server-project:9000/default
      PYICEBERG_HOME: /var/lib/clickhouse/user_files
      PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://messud-bucket/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: minioadmin
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: minioadmin
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./sample_data:/var/lib/clickhouse/user_files
      - ./dbt_project:/dbt
    depends_on:
      - airflow-init-project
      - airflow-db-project
      - clickhouse-server-project
    restart: always
    networks:
      iceberg_network:

  airflow-scheduler-project:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler-project
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db-project:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: clickhouse+native://default:12345678@clickhouse-server-project:9000/default
      PYICEBERG_HOME: /var/lib/clickhouse/user_files
      PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://messud-bucket/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: minioadmin
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: minioadmin
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./sample_data:/var/lib/clickhouse/user_files
      - ./dbt_project:/dbt
    depends_on:
      - airflow-init-project
      - airflow-db-project
      - clickhouse-server-project
    restart: always
    networks:
      iceberg_network:

volumes:
  pgdata_airflow-project:
  clickhouse_data-project:
  minio_data:
